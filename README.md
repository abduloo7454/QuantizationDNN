# QuantizationDNN


| TITLE | YEAR | CODE | PAPER LINK |
|-------|------|------|------|
| A Survey on Transformer Compression   |   2024   |  -    |    [paper](https://arxiv.org/abs/2402.05964)   |
|A Comprehensive Survey on Model Quantization for Deep Neural Networks in Image Classification | 2023 | [code](https://github.com/NVIDIA/FasterTransformer) |    [paper](https://dl.acm.org/doi/10.1145/3623402)    |
|  A Survey of Quantization Methods for Efficient Neural Network Inference  | 2021 |  -  |    [paper](https://arxiv.org/abs/2103.13630)    |
| Model Quantization and Hardware Acceleration for Vision Transformers: A Comprehensive Survey |  2024  |  [code](https://github.com/DD-DuDa/awesome-vit-quantization-acceleration)  |  [paper](https://arxiv.org/abs/2405.00314)  |
| Quantization and Deployment of Deep Neural Networks on Microcontrollers | 2021 | - |   [paper](https://arxiv.org/abs/2105.13331)  |
| COAT: COMPRESSING OPTIMIZER STATES AND ACTIVATION FOR MEMORY-EFFICIENT FP8 TRAINING | 2024 | [code](https://github.com/NVlabs/COAT) |  [paper](https://arxiv.org/abs/2410.19313)   |
| Compressing Large Language Models using Low Rank and Low Precision Decomposition | 2024 | [code](https://github.com/pilancilab/caldera) |  [paper](https://arxiv.org/abs/2405.18886) |
|     |     |     |                     |
|     |     |     |  |
